# -*- coding: utf-8 -*-
"""Ensemble.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mcTt6Hh17x9q5x5Pr7efFhRUgxBVaXb7
"""
import itertools
from collections import defaultdict

import numpy as np
import pandas as pd
import recordlinkage
from IPython import display
from numpy import dot
from numpy.linalg import norm
from sklearn.model_selection import ParameterGrid
from tqdm.auto import tqdm


class ActiveLearner:
  """
	class to implement self and active learning

	Methods
	------
	train()

		trains the model

		labels high confidence pairs and adds to training data while
      	maintaining approximate class balance

  		pulls out low confidence pairs for clerical review

	clerical_review()

		used to manually label uncertain pairs and adds them to training data
  """
  def __init__(self, df_org1, df_org2, X_labeled, y_labeled, X_unlabeled, model, n_certain, n_uncertain):

      """
   	Parameters
   	----------
   	df_org1 : dataframe
   		first of two datasets for record linkage
   	df_org2 : dataframe
   		second of two datasets for record linkage
   	X_labeled : dataframe
   		labeled data for training with labels removed
   	y_labeled : series
   		labels for training data
   	X_unlabeled : dataframe
   		unlabeled data
   	model : sklearn classifier
   		classifier that has .fit, .predict and .predict_proba methods
   	n_certain : int
   		how many high confidence pairs to add to training data during each iteration
   	n_uncertain : int
   		how many low confidence pairs to label by hand during each iteration

      """
      self.original_data1 = df_org1.copy()
      self.original_data2 = df_org2.copy()
      self.X_train = X_labeled.copy()
      self.y_train = y_labeled.copy()
      self.X_nolabel = X_unlabeled.copy()
      self.model = model
      self.n_certain_true = int(np.ceil(n_certain * np.mean(self.y_train)))
      self.n_certain_false = int(n_certain - self.n_certain_true)
      self.n_uncertain = n_uncertain
      self.df_uncertain = pd.DataFrame()


  def _entropy(self, p):
    '''helper function to compute entropy'''
    return -p * np.log(p) - (1-p) * np.log(1-p)

  def train(self):
    """method used to train during each iteration
        The highest confidence pairs are labeled and added to training data
        both high confidence True and False pairs are labeled such that approximate
        class balance is maintained

      	The lowest confidence pairs can be reviewed using the clerical_review method

      	In the case where the highest and lowest confidence pairs intersect (this generally happens
       	when the highest confidence positive pairs still are fairly low confidence) no high confidence
      	pairs are automatically added to training data and we rely on clerical_review method
    """

    print('pre train')
    print(' training data shape: ', self.X_train.shape)
    print(' training labels shape: ', self.y_train.shape)
    print(' unlabeled data shape: ', self.X_nolabel.shape)

    # fit model with labeled data
    self.model.fit(self.X_train, self.y_train)

    # predict proba with data with no label
    probs = self.model.predict_proba(self.X_nolabel)

    # pull out prob of class 1
    probs = [prob[1] for prob in probs]

    # sort by entropy and pull out uncertain pairs
    if self.n_uncertain:
      ent_sorted = sorted([(self._entropy(prob), ind) for prob, ind in zip(probs, self.X_nolabel.index)])
      uncertain = [ind for _, ind in ent_sorted[-self.n_uncertain:]]

    # sort by prob and pull out certain pairs such that the class balance of the
    # initial training data is preserved
    prob_sorted = sorted([(prob, ind) for prob, ind in zip(probs, self.X_nolabel.index)])

    #n_certain_false low probability pairs and n_certain_true high prob pairs are pulled out
    certain = [ind for _, ind in prob_sorted[:self.n_certain_false]] + [ind for _, ind in prob_sorted[-self.n_certain_true:]]

    # create df_uncertain pairs
    if self.n_uncertain:
      self.df_uncertain = self.X_nolabel.loc[uncertain, : ]

    # check if certain and uncertain intersect
    # this happens in the case where our most confident positive cases still have very low prob
    # in this situation we don't add the most certain cases - instead we rely on the clerical_review function
    cardinality = len([i for i in uncertain if i in set(certain)])
    if cardinality == 0:
        # add certain pairs to training data
        df_certain = self.X_nolabel.loc[certain, : ]
        self.X_train = self.X_train.append(df_certain)
        y_certain = pd.Series(data=self.model.predict(df_certain), index=df_certain.index)
        self.y_train = self.y_train.append(y_certain)

        # remove certain pairs from nolabel data
        self.X_nolabel.drop(df_certain.index, inplace=True)

    # if prop of most confident pair is very low instruct user to perform clerical review
    if cardinality > 0:
      print('prob of most confident examples was very low!')
      print('no pairs were added to traning data')
      print('use clerical_review function')

    print('post train')
    print(' training data shape: ', self.X_train.shape)
    print(' training labels shape: ', self.y_train.shape)
    print(' unlabeled data shape: ', self.X_nolabel.shape)

    # notify user if active learning is set to false
    if not self.n_uncertain:
      print('n_uncertain == False')
      print('if clerical review is required set n_uncertain to number of uncertain pairs to label (int)')


  def clerical_review(self):
    '''review uncertain pairs'''

    # bool to keep track of early exit
    early_exit = False

    # keep track of pairs that are labeled in clerical review
    labeled_list = []
    labeled_list_y = []

    # return none if n_uncertain is set to False
    if not self.n_uncertain:
      print('n_uncertain set to False')
      return None

    # iterate through rows of df_uncertain
    if self.df_uncertain.shape[0] == 0:
      print('df is empty')
      return None
    for i in range(self.df_uncertain.shape[0]):
      while True:
          # pull out indices of pair and display data from original dfs
          org1_index, org2_index = self.df_uncertain.index[i]
          display(pd.concat([self.original_data1.loc[org1_index,:],
                            self.original_data2.loc[org2_index, :]], axis=1).T)

          # label the data True/False or quit
          label = input("Enter label (True/False) :")
          if label == 'quit':
            early_exit = True
            break
          elif label == 'True':
            label = True
          elif label == 'False':
            label = False

          if (label == True) or (label == False):
            # add labeled pair to labeled_list
            labeled_list.append(self.df_uncertain.index[i])
            labeled_list_y.append(label)
            break

          else:
            print('invalid entry - try again \n enter "quit" to terminate loop')

      if label == 'quit':
          break
    # add pair to X_train, y_train and remove from X_nolabel
    self.X_train = self.X_train.append(self.df_uncertain.loc[labeled_list])
    y_labeled = pd.Series(data=labeled_list_y,
                        index=labeled_list)
    self.y_train = self.y_train.append(y_labeled)
    self.X_nolabel.drop(labeled_list, axis=0, inplace=True)
    self.X_nolabel


    # remove classified observations from df_uncertain
    # if loop terminated before all pairs are classified these pairs remain in df_uncertain
    self.df_uncertain = self.df_uncertain.drop(labeled_list)

    if early_exit:
      print('loop terminated early')

    else:
      print('all data labeled')


import itertools
from collections import defaultdict

import numpy as np
import pandas as pd
import recordlinkage
from numpy import dot
from numpy.linalg import norm
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import ParameterGrid
from tqdm.auto import tqdm


class Ensemble:

  """
	class to implement ensemble classifier

	Methods
	------
	measurement_scheme()

		computes all the combinations of distance measures that will be used in
    measurement scheme dataframes

	build_schemes()

    computes the dataframes of distance measures for each pairwise comparison
    for each distance measure scheme

  train_ensemble()

    used to train a model for each of the distance measure dataframes

  pred_ensemble()

    used to predict the modal response for all base classifiers

  """

  def __init__(self, df1, df2, candidate_pairs):
    """
   	Parameters
   	----------
   	df_1 : dataframe
   		first of two datasets for record linkage

   	df_2 : dataframe
   		second of two datasets for record linkage

    candidate_pairs : pd.MultiIndex
      pd.MultiIndex of the candidate_pairs that need
      to be compared in df_1 and df_2

    """

    self.df1 = df1.copy()
    self.df2 = df2.copy()
    self.candidate_pairs = candidate_pairs
    self.final_measures = None
    self.compare_dict = None
    self.classifier_dict = {}
    self.preds_df = None

  def measurement_scheme(self, p: int, return_:bool) -> dict:
    '''
    computes the schemes of distance measures

    Parameters
    ----------
    p : int
      parameter which determines number of distance measures to use on each
      field

    return_ : bool
      determines whether a measurement schemes will be returned or just saved as
      self.final_measures

    Returns
    -------
    dict or None depending on value of return_

    '''
    def _cos_sim(a, b):

      '''computes cos similarity'''

      return dot(a, b)/(norm(a)*norm(b))

    def _def_value2():

      '''set default value for dict'''

      return 0

    # dict to save all distance measure schemes
    final_measures = {}

    # columns to iterate through
    cols = self.df1.columns

    # distance measures supported by recordlinkage
    string_measures = ['jaro','jarowinkler', 'levenshtein', 'damerau_levenshtein', 'qgram','cosine']

    # compute each similarity measure for each column
    for col in cols:
      similarity_vecs = {}
      for measure in string_measures:
        comp = recordlinkage.Compare()
        comp.string(col, col, method=measure)
        df_compare = comp.compute(self.candidate_pairs, self.df1, self.df2)
        similarity_vecs[measure] = df_compare.loc[:,0].values

      # dict to store similarity measures of similarity measures for the current column
      scores = defaultdict(_def_value2)

      # original list of string measures
      V_f = string_measures.copy()

      # empty list to add best measures to
      V_f_hat = []

      # compute cos sim for each combination of similarity vecs
      for k1, k2 in itertools.combinations(similarity_vecs.items(), 2):
        sim = _cos_sim(k1[1], k2[1])
        scores['-'.join(sorted([k1[0], k2[0]]))] = sim

      # find pair with min cos sim
      min_pair = min(scores.keys(), key=(lambda k: scores[k])).split('-')

      # add measures to V_f_hat, remove from V_f (candidate measures)
      V_f_hat.append(min_pair[0])
      V_f_hat.append(min_pair[1])
      V_f.remove(min_pair[0])
      V_f.remove(min_pair[1])


      # iterate through selection process till V_f (candidate measures) is empty
      while len(V_f) > 0:

        # remove all measure from V_f that have score with measure in V_f_hat > p
        remove_list = [m1 for m1 in V_f for m2 in V_f_hat if scores['-'.join(sorted([m1, m2]))] > p]
        for m in list(set(remove_list)):
          V_f.remove(m)

        # create list of similarity scores between all elements in V_f and V_f_hat
        m_list = [(m1, scores['-'.join(sorted([m1, m2]))]) for m1 in V_f for m2 in V_f_hat ]

        # break if empty
        if len(m_list) == 0:
          break

        # add measure with highest similarity score to V_f_hat
        # and remove from V_f
        m_to_add = max(m_list, key=lambda score: score[1])[0]
        V_f_hat.append(m_to_add)
        V_f.remove(m_to_add)

      # add all measures in V_f_hat to final_measures
      final_measures[col] = V_f_hat
    self.final_measures = final_measures
    if return_:
      return final_measures

  def build_schemes(self,return_:bool, n=True) -> dict:
    '''
    computes the dataframes of distance measures for each pairwise comparison
    for each distance measure scheme

    Parameters
    ----------
    return_ : bool
      determines whether the dataframes will be returned or just saved as
      self.compare_dict

    n : int, optional
      if parameter is give it only computes the first n measurement schemes
      when training a final model this parameter should not be passed into the function,
      parameter can be used during experimentation with\demonstration of Ensemble class
      because this method can be very time consuming if run in its entirety

    Returns
    -------
    dict of dataframes or None depending on value of return_

    '''
    # create param grid for measurement schemes
    param_grid = ParameterGrid(self.final_measures)

    # create df for each measurement scheme and save in self.compare_dict
    compare_dict = {}
    for i, dict_ in tqdm(enumerate(param_grid)):
      if (n != True) and (i == n):
        break
      comp = recordlinkage.Compare()
      for col in self.df1.columns:
          comp.string(col, col, method=dict_[col])
      df_compare = comp.compute(self.candidate_pairs, self.df1, self.df2)
      compare_dict[i] = df_compare
    self.compare_dict = compare_dict
    if return_:
      return compare_dict

  def train_ensemble(self, indices_train:pd.MultiIndex, y_train:pd.Series, model_list:list) -> None:
    '''
    trains the base classifiers based on the dataframes computed in the build_schemes method

    Parameters
    ----------
    indices_train : pd.MultiIndex
      indices for the training data that corresponds with the indices in the
      original df1 and df2 passed into the Ensemble class

    y_train : pd.Series
      pd.Series of labels for indices_train

    model_list : list
      list of sklearn classifiers
      len(model_list) should equal len(self.compare_dict)

    Returns
    -------
    None

    '''
    # iterate through each measurement scheme df, train model, save to self.classifier_dict
    for i, tup in tqdm(enumerate(zip(model_list, self.compare_dict.values()))):
      model, dict_ = tup[0], tup[1]
      X_train_ = dict_.loc[indices_train]
      model.fit(X_train_, y_train)
      self.classifier_dict[i] = model

  def pred_ensemble(self, indices_test:pd.MultiIndex) -> np.ndarray:
    '''
    used to predict the modal response for all base classifiers

    Parameters
    ----------
    indices_test
      indices for unlabeled data to be predicted,
      should be the compliment of indices_train

    Returns
    -------
    np.ndarray
      returns all the predictions for unlabeled data

    '''
    # dict to store results of predictions
    pred_dict = {}

    # iterate through each classifier/measurement scheme df and predict
    for i, tup in enumerate(zip(self.classifier_dict.values(), self.compare_dict.values())):
      model, data = tup[0], tup[1]
      y_pred = model.predict(data.loc[indices_test])
      pred_dict[i] = y_pred

    # create df of all predictions
    self.preds_df = pd.DataFrame(pred_dict)

    # return mode
    return self.preds_df.mode(axis=1)[0].values





